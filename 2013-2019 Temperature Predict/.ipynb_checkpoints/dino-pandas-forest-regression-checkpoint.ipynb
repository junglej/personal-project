{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the **data**! The data that doctors should use to diagnose potential breast cancer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the **labels**: The describe whether patients with above symptosm did end having cancer. `1`s are cancer, `0`s are no cancer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0\n",
       "5      0\n",
       "6      0\n",
       "7      0\n",
       "8      0\n",
       "9      0\n",
       "10     0\n",
       "11     0\n",
       "12     0\n",
       "13     0\n",
       "14     0\n",
       "15     0\n",
       "16     0\n",
       "17     0\n",
       "18     0\n",
       "19     1\n",
       "20     1\n",
       "21     1\n",
       "22     0\n",
       "23     0\n",
       "24     0\n",
       "25     0\n",
       "26     0\n",
       "27     0\n",
       "28     0\n",
       "29     0\n",
       "      ..\n",
       "539    1\n",
       "540    1\n",
       "541    1\n",
       "542    1\n",
       "543    1\n",
       "544    1\n",
       "545    1\n",
       "546    1\n",
       "547    1\n",
       "548    1\n",
       "549    1\n",
       "550    1\n",
       "551    1\n",
       "552    1\n",
       "553    1\n",
       "554    1\n",
       "555    1\n",
       "556    1\n",
       "557    1\n",
       "558    1\n",
       "559    1\n",
       "560    1\n",
       "561    1\n",
       "562    0\n",
       "563    0\n",
       "564    0\n",
       "565    0\n",
       "566    0\n",
       "567    0\n",
       "568    1\n",
       "Length: 569, dtype: int32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MORE COMPLEX VERSION, MAY NEED PROFESSOR-LEVEL PACKAGES**\n",
    "\n",
    "If you're missing packages, then jump cells below and straight to the cell marked **RUN HERE FOR SIMPLE STANDALONE VERSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active code page: 936\n",
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\17931\\anaconda3\\lib\\site-packages (from sklearn) (0.21.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\17931\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\17931\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.16.4)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\17931\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\17931\\AppData\\Local\\pip\\Cache\\wheels\\76\\03\\bb\\589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n",
      "Active code page: 936\n",
      "Collecting sklearn.grid_search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Could not find a version that satisfies the requirement sklearn.grid_search (from versions: none)\n",
      "ERROR: No matching distribution found for sklearn.grid_search\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn\n",
    "!pip install sklearn.grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import our curve-building algorithm using tree-logic as explained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_grid = dict(\n",
    "    max_features=np.arange(2, 8),\n",
    "    max_depth=[2, 4],\n",
    "    min_samples_split=[5, 10, 15, 20],\n",
    ")\n",
    "rfc = RandomForestRegressor(n_estimators=10)\n",
    "gs = GridSearchCV(rfc, param_grid, cv=5, n_jobs=-1)\n",
    "gs.fit(X.values, y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[mean: 0.71057, std: 0.09425, params: {'max_depth': 2, 'max_features': 2, 'min_samples_split': 5},\n",
       " mean: 0.72736, std: 0.08405, params: {'max_depth': 2, 'max_features': 2, 'min_samples_split': 10},\n",
       " mean: 0.75531, std: 0.06236, params: {'max_depth': 2, 'max_features': 2, 'min_samples_split': 15},\n",
       " mean: 0.69665, std: 0.08930, params: {'max_depth': 2, 'max_features': 2, 'min_samples_split': 20},\n",
       " mean: 0.72999, std: 0.10873, params: {'max_depth': 2, 'max_features': 3, 'min_samples_split': 5},\n",
       " mean: 0.73250, std: 0.11170, params: {'max_depth': 2, 'max_features': 3, 'min_samples_split': 10},\n",
       " mean: 0.72664, std: 0.10219, params: {'max_depth': 2, 'max_features': 3, 'min_samples_split': 15},\n",
       " mean: 0.74243, std: 0.10487, params: {'max_depth': 2, 'max_features': 3, 'min_samples_split': 20},\n",
       " mean: 0.75516, std: 0.11585, params: {'max_depth': 2, 'max_features': 4, 'min_samples_split': 5},\n",
       " mean: 0.73956, std: 0.12662, params: {'max_depth': 2, 'max_features': 4, 'min_samples_split': 10}]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = gs.grid_scores_\n",
    "scores[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the features that doctors ought to be paying attention to in order to diagnose cancer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ed05280617b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m pd.Series(gs.best_estimator_.feature_importances_,\n\u001b[0m\u001b[0;32m      2\u001b[0m           index=X.columns).sort_values().plot(kind='barh')\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gs' is not defined"
     ]
    }
   ],
   "source": [
    "pd.Series(gs.best_estimator_.feature_importances_,\n",
    "          index=X.columns).sort_values().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN HERE FOR SIMPLE STANDALONE VERSION\n",
    "\n",
    "This is the simpler version of the regression forest algorithm that is likely to work very well without dependencies. It's what we used last class.\n",
    "\n",
    "We split a dataset into a training bundle and a testing bundle. We train a Machine Learning algorithm on the training bundle, and then we test on the test bundle to see *if it guesses right*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(X.values, y.values, test_size = 0.25,\n",
    "                                                                           random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=42, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Instantiate model \n",
    "rf = RandomForestRegressor(n_estimators= 10, random_state=42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the guess in error on the test bundle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 0.07 degrees.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(test_features)\n",
    "\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - test_labels)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wowwwwww... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean radius',\n",
       " 'mean texture',\n",
       " 'mean perimeter',\n",
       " 'mean area',\n",
       " 'mean smoothness',\n",
       " 'mean compactness',\n",
       " 'mean concavity',\n",
       " 'mean concave points',\n",
       " 'mean symmetry',\n",
       " 'mean fractal dimension',\n",
       " 'radius error',\n",
       " 'texture error',\n",
       " 'perimeter error',\n",
       " 'area error',\n",
       " 'smoothness error',\n",
       " 'compactness error',\n",
       " 'concavity error',\n",
       " 'concave points error',\n",
       " 'symmetry error',\n",
       " 'fractal dimension error',\n",
       " 'worst radius',\n",
       " 'worst texture',\n",
       " 'worst perimeter',\n",
       " 'worst area',\n",
       " 'worst smoothness',\n",
       " 'worst compactness',\n",
       " 'worst concavity',\n",
       " 'worst concave points',\n",
       " 'worst symmetry',\n",
       " 'worst fractal dimension']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving feature names for later use\n",
    "feature_list = list(X.columns)\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the synptoms that doctors should be paying attention to, prioritized, in order to correctly diagnose breast cancer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: worst perimeter      Importance: 0.3\n",
      "Variable: worst concave points Importance: 0.22\n",
      "Variable: worst radius         Importance: 0.16\n",
      "Variable: worst area           Importance: 0.11\n",
      "Variable: mean concave points  Importance: 0.08\n",
      "Variable: worst texture        Importance: 0.03\n",
      "Variable: mean texture         Importance: 0.02\n",
      "Variable: area error           Importance: 0.02\n",
      "Variable: worst concavity      Importance: 0.02\n",
      "Variable: concavity error      Importance: 0.01\n",
      "Variable: concave points error Importance: 0.01\n",
      "Variable: mean radius          Importance: 0.0\n",
      "Variable: mean perimeter       Importance: 0.0\n",
      "Variable: mean area            Importance: 0.0\n",
      "Variable: mean smoothness      Importance: 0.0\n",
      "Variable: mean compactness     Importance: 0.0\n",
      "Variable: mean concavity       Importance: 0.0\n",
      "Variable: mean symmetry        Importance: 0.0\n",
      "Variable: mean fractal dimension Importance: 0.0\n",
      "Variable: radius error         Importance: 0.0\n",
      "Variable: texture error        Importance: 0.0\n",
      "Variable: perimeter error      Importance: 0.0\n",
      "Variable: smoothness error     Importance: 0.0\n",
      "Variable: compactness error    Importance: 0.0\n",
      "Variable: symmetry error       Importance: 0.0\n",
      "Variable: fractal dimension error Importance: 0.0\n",
      "Variable: worst smoothness     Importance: 0.0\n",
      "Variable: worst compactness    Importance: 0.0\n",
      "Variable: worst symmetry       Importance: 0.0\n",
      "Variable: worst fractal dimension Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get numerical feature importances\n",
    "importances = list(rf.feature_importances_)\n",
    "\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "# Print out the feature and importances \n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This is exactly what IBM's Watson is doing (albeit with ***a lot*** more data) and fooling the world into thinking it is some kind of Artificial Intelligence. Nope, it's just **statistics**.\n",
    "\n",
    "What dataset would you like to apply this to, dress up like a magician, and amaze your friends?\n",
    "\n",
    "<br />\n",
    "<center>\n",
    "    <img src=\"ipynb.images/fortune-teller-reading-crystal-ball.jpg\" width=600 />\n",
    "</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
